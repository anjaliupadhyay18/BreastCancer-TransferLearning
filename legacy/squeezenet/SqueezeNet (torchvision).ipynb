{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breast tumour classification with SqueezeNet\n",
    "=============================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SqueezeNet 1.0:  \n",
    "With training the model from scratch without any pre-training, SqueezeNet has been able to achieve **59.2%** validation accuracy.  \n",
    "With freezing and training the final layers, SqueezeNet has been able to achieve **66.7%** validation accuracy.  \n",
    "With training the entire model after initalising with pre-training weights, SqueezeNet has been able to achieve **82.2%** validation accuracy.\n",
    "\n",
    "Using SqueezeNet 1.1:  \n",
    "With training the entire model after initalising with pre-training weights, SqueezeNet has been able to achieve **81.5%** validation accuracy in 41 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial parameters\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/Renee/Documents/2019/Semester 2/COMP5703/Data/breakhis\"\n",
    "\n",
    "num_classes = 8\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# False to finetune the whole model. True to freeze and update only last layers\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    start = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                torch.save(model, 'torchvision_squeezenet.pt')\n",
    "                \n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def initialise_model(num_classes, feature_extract, use_pretrained=True):\n",
    "\n",
    "    # using squeezenet 1.0\n",
    "    model_ft = models.squeezenet1_0(pretrained=use_pretrained) #change this line to change model\n",
    "    \n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "    model_ft.num_classes = num_classes\n",
    "    input_size = 224\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialise model\n",
    "model_ft, input_size = initialise_model(num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_by_class(train_imgs, num_classes):                        \n",
    "    samples = [0] * num_classes                                                      \n",
    "    for img in train_imgs:                                                         \n",
    "        samples[img[1]] += 1  \n",
    "        \n",
    "    weights = [0.] * num_classes                                                                \n",
    "    for i in range(num_classes):                                                   \n",
    "        weights[i] = float(sum(samples))/float(samples[i])\n",
    "        \n",
    "    sample_weights = [0] * len(train_imgs)                                              \n",
    "    for idx, val in enumerate(train_imgs):                                          \n",
    "        sample_weights[idx] = weights[val[1]]                                  \n",
    "    return sample_weights, weights       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights, class_weights = samples_by_class(image_datasets['train'], num_classes)\n",
    "sample_weights = torch.DoubleTensor(sample_weights).to(device)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights, len(sample_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t features.0.weight\n",
      "\t features.0.bias\n",
      "\t features.3.squeeze.weight\n",
      "\t features.3.squeeze.bias\n",
      "\t features.3.expand1x1.weight\n",
      "\t features.3.expand1x1.bias\n",
      "\t features.3.expand3x3.weight\n",
      "\t features.3.expand3x3.bias\n",
      "\t features.4.squeeze.weight\n",
      "\t features.4.squeeze.bias\n",
      "\t features.4.expand1x1.weight\n",
      "\t features.4.expand1x1.bias\n",
      "\t features.4.expand3x3.weight\n",
      "\t features.4.expand3x3.bias\n",
      "\t features.5.squeeze.weight\n",
      "\t features.5.squeeze.bias\n",
      "\t features.5.expand1x1.weight\n",
      "\t features.5.expand1x1.bias\n",
      "\t features.5.expand3x3.weight\n",
      "\t features.5.expand3x3.bias\n",
      "\t features.7.squeeze.weight\n",
      "\t features.7.squeeze.bias\n",
      "\t features.7.expand1x1.weight\n",
      "\t features.7.expand1x1.bias\n",
      "\t features.7.expand3x3.weight\n",
      "\t features.7.expand3x3.bias\n",
      "\t features.8.squeeze.weight\n",
      "\t features.8.squeeze.bias\n",
      "\t features.8.expand1x1.weight\n",
      "\t features.8.expand1x1.bias\n",
      "\t features.8.expand3x3.weight\n",
      "\t features.8.expand3x3.bias\n",
      "\t features.9.squeeze.weight\n",
      "\t features.9.squeeze.bias\n",
      "\t features.9.expand1x1.weight\n",
      "\t features.9.expand1x1.bias\n",
      "\t features.9.expand3x3.weight\n",
      "\t features.9.expand3x3.bias\n",
      "\t features.10.squeeze.weight\n",
      "\t features.10.squeeze.bias\n",
      "\t features.10.expand1x1.weight\n",
      "\t features.10.expand1x1.bias\n",
      "\t features.10.expand3x3.weight\n",
      "\t features.10.expand3x3.bias\n",
      "\t features.12.squeeze.weight\n",
      "\t features.12.squeeze.bias\n",
      "\t features.12.expand1x1.weight\n",
      "\t features.12.expand1x1.bias\n",
      "\t features.12.expand3x3.weight\n",
      "\t features.12.expand3x3.bias\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation dataloaders\n",
    "#dataloaders_dict = {\n",
    "#    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "#    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, num_workers=0)\n",
    "#}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train','val']}\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Select which parameters to learn\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Validation\n",
    "--------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 0.5782 Acc: 0.7514\n",
      "val Loss: 0.7885 Acc: 0.6217\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.5803 Acc: 0.7379\n",
      "val Loss: 0.5156 Acc: 0.7882\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.5536 Acc: 0.7543\n",
      "val Loss: 0.5918 Acc: 0.7102\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.5769 Acc: 0.7500\n",
      "val Loss: 0.4927 Acc: 0.7608\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.5241 Acc: 0.7665\n",
      "val Loss: 0.5785 Acc: 0.7935\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.4972 Acc: 0.7843\n",
      "val Loss: 0.4642 Acc: 0.7629\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.5275 Acc: 0.7710\n",
      "val Loss: 0.5158 Acc: 0.7671\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.5108 Acc: 0.7705\n",
      "val Loss: 0.4266 Acc: 0.8019\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.5157 Acc: 0.7733\n",
      "val Loss: 0.4948 Acc: 0.8251\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.4609 Acc: 0.7898\n",
      "val Loss: 0.8406 Acc: 0.6828\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.4846 Acc: 0.7807\n",
      "val Loss: 0.4866 Acc: 0.7798\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.4625 Acc: 0.7922\n",
      "val Loss: 0.5328 Acc: 0.7671\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.4593 Acc: 0.7947\n",
      "val Loss: 0.5228 Acc: 0.7935\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.4423 Acc: 0.8017\n",
      "val Loss: 0.4789 Acc: 0.7914\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.4863 Acc: 0.7841\n",
      "val Loss: 0.5313 Acc: 0.7819\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.4386 Acc: 0.8078\n",
      "val Loss: 0.6675 Acc: 0.8166\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.4051 Acc: 0.8126\n",
      "val Loss: 0.6642 Acc: 0.7640\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.4328 Acc: 0.8096\n",
      "val Loss: 0.6297 Acc: 0.7165\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.4263 Acc: 0.8030\n",
      "val Loss: 0.4114 Acc: 0.8219\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.3994 Acc: 0.8188\n",
      "val Loss: 0.4880 Acc: 0.7987\n",
      "\n",
      "Training complete in 32m 52s\n",
      "Best val Acc: 0.825079\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 965M\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "#del model_ft\n",
    "#gc.collect()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model from scratch\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the non-pretrained version of the model used for this run\n",
    "scratch_model,_ = initialise_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves of validation accuracy vs. number \n",
    "#  of training epochs for the transfer learning method and\n",
    "#  the model trained from scratch\n",
    "p_hist = []\n",
    "s_hist = []\n",
    "\n",
    "p_hist = [h.cpu().numpy() for h in hist]\n",
    "s_hist = [h.cpu().numpy() for h in scratch_hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),p_hist,label=\"Pretrained\")\n",
    "plt.plot(range(1,num_epochs+1),s_hist,label=\"Scratch\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B_A', 'B_F', 'B_PT', 'B_TA', 'M_DC', 'M_LC', 'M_MC', 'M_PC']\n",
      "tensor([[ 49.,   0.,   0.,   0.,   0.,   0.,   2.,   0.],\n",
      "        [  2.,  94.,   8.,   7.,   4.,   0.,   3.,   3.],\n",
      "        [  0.,   4.,  39.,   1.,   1.,   1.,   2.,   2.],\n",
      "        [  1.,   9.,   0.,  58.,   0.,   0.,   0.,   2.],\n",
      "        [  6.,   4.,   0.,   1., 348.,  22.,  15.,  23.],\n",
      "        [  0.,   0.,   0.,   0.,  22.,  47.,   1.,   3.],\n",
      "        [  1.,   2.,   0.,   4.,   4.,   0.,  84.,   2.],\n",
      "        [  0.,   0.,   0.,   0.,   4.,   0.,   0.,  64.]])\n",
      "tensor([0.9608, 0.7769, 0.7800, 0.8286, 0.8305, 0.6438, 0.8660, 0.9412])\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(dataloaders_dict['val']):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_ft(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "print(image_datasets['train'].classes)\n",
    "print(confusion_matrix)\n",
    "\n",
    "# to get percentage for each class\n",
    "print(confusion_matrix.diag()/confusion_matrix.sum(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
